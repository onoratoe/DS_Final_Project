{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Phishing Classificaiton</h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>n_period</th>\n",
       "      <th>n_hyphens</th>\n",
       "      <th>n_underscore</th>\n",
       "      <th>n_slash</th>\n",
       "      <th>n_questionmrk</th>\n",
       "      <th>n_equals</th>\n",
       "      <th>n_at</th>\n",
       "      <th>n_and</th>\n",
       "      <th>n_exclamation</th>\n",
       "      <th>url_length</th>\n",
       "      <th>domain_name_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   status  n_period  n_hyphens  n_underscore  n_slash  n_questionmrk  \\\n",
       "0     0.0         2          0             0        3              0   \n",
       "1     0.0         2          1             0        3              0   \n",
       "2     0.0         2          1             0        0              0   \n",
       "3     0.0         1          0             0        0              0   \n",
       "4     0.0         3          0             0       10              1   \n",
       "5     0.0         3          2             0        0              0   \n",
       "6     0.0         3          0             0        7              0   \n",
       "7     0.0         2          1             1        7              0   \n",
       "8     0.0         5          2             0        3              0   \n",
       "9     0.0         4          0             0        0              0   \n",
       "\n",
       "   n_equals  n_at  n_and  n_exclamation  url_length  domain_name_correct  \n",
       "0         0     0      0              0          25                    1  \n",
       "1         0     0      0              0          35                    1  \n",
       "2         0     0      0              0          35                    1  \n",
       "3         0     0      0              0           9                    0  \n",
       "4         0     0      0              0          74                    0  \n",
       "5         0     0      0              0          36                    0  \n",
       "6         0     0      0              0          54                    0  \n",
       "7         0     0      0              0          74                    1  \n",
       "8         0     0      0              0          50                    1  \n",
       "9         0     0      0              0          21                    1  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('parsed_url_features.csv')\n",
    "# drop non numeric columns\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df = df.drop(col, axis=1)\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  347   447   571   610   675   703   715   724  1012  1066  1176  1239\n",
      "  1245  1280  1468  1643  1734  1940  1981  2063  2095  2128  2208  2210\n",
      "  2379  2595  2636  2678  2785  3040  3047  3081  3280  3445  3470  3531\n",
      "  3589  3730  3779  3817  3863  3917  4039  4175  4682  4773  4784  5012\n",
      "  5092  5097  5164  5318  5360  5416  5569  5751  6009  6070  6345  6545\n",
      "  6907  6924  6956  6994  7541  7723  7885  7900  7996  8014  8044  8046\n",
      "  8163  8362  8480  8538  8567  8669  8795  8823  8856  8860  9029  9115\n",
      "  9513  9516  9843  9899 10172 10196 10406 10536 10560 10759 10766 10881\n",
      " 10942 11054 11247 11288 11595 11732 11835 12054 12061 12322 12378 12644\n",
      " 12669 12727 12733 12922 13167 13304 13356 13587 13616 13682 13790 13892\n",
      " 14042 14135 14202 14270 14678 14786 15027 15133 15233 15264 15318 15521\n",
      " 15651 15785 15799 15815 15840 15990 16000 16035 16310 16328 16336 16381\n",
      " 16394 16441 16481 16489 16589 16608 16680 16837 17009 17060 17298 17442\n",
      " 17452 17545 17571 17839 17887 18054 18186 18189 18217 18243 18274 18755\n",
      " 18999 19300 19510 19926 20006 20126 20211 20241 20296 20347 20386 20400\n",
      " 20479 20537 20553 20613 20615 20890 21422 21453 21728 21757 21813 22016\n",
      " 22043 22181 22188 22249 22459 22470 22615 22819 22834 22987 22989 23044\n",
      " 23155 23345 23372 23414 23498 23624 23682 23789 23965 24253 24277 24278\n",
      " 24295 24346 24447 24492 24593 24633 24988 25013 25023 25076 25269 25405\n",
      " 25442 25449 25551 25867 25944 25984 26148 26207 26511 26540 26624 26990\n",
      " 27378 27667 27703 27865 27914 27931 27968 28076 28096 28125 28258 28272\n",
      " 28349 28561 28685 28724 28782 28787 28822 28953 28983 29159 29172 29260\n",
      " 29289 29309 29616 29735 29793 29812 29825 29828 29924 30077 30136 30257\n",
      " 30270 30279 30292 30309 30472 30618 30665 30799 30807 30823 30982 31048\n",
      " 31146 31151 31180 31507 31765 32181 32184 32438 32600 32647 32696 32858\n",
      " 32921 33007 33136 33143 33252 33292 33345 33374 33507 33746 33774 33851\n",
      " 33958 34243 34596 34612 34712 34946 35104 35161 35232 35235 35272 35364\n",
      " 35492 35529 35576 35598 36236 36375 36442 36470 36628 36797 37245 37247\n",
      " 37475 37485 37525 37553 37604 37607 37627 37666 37712 37800 38052 38090\n",
      " 38145 38327 38339 38343 38579 38664 38688 38782 38855 38892 38938 38983\n",
      " 39012 39167 39424 39588 39593 39731 39829 39892 39902 40157 40167 40196\n",
      " 40367 40394 40658 40705 40718 40747 40958 41101 41302 41342 41396 41401\n",
      " 41590 41653 41668 41878 41926 41959 42051 42142 42293 42375 42543 42812\n",
      " 43106 43149 43205 43207 43261 43268 43277 43382 43543 43566 43587 43631\n",
      " 43700 43765 44042 44109 44162 44328 44376 44588 44629 44725 44754 44858\n",
      " 44984 45162 45175 45242 45282 45435 45563 45618 45684 45751 45784 45839\n",
      " 45860 45876 45933 45987 46049 46079 46185 46262 46293 46340 46505 46550\n",
      " 46647 46797 46938 46971 47251 47283 47294 47455 47528 47543 47588 47596\n",
      " 47672 47788 47971 49905 56170 56841 57038 61349 61987 65418 65843 66548\n",
      " 68526 69768 70010 70044 70493 71036 71070 75891 75950 77783 79768 79804\n",
      " 80963 83065 84917 85357 89661 91400 91880 94030 94768 95519 96222 96246\n",
      " 96930]\n",
      "Number of outliers: 493\n",
      "(98572, 12)\n"
     ]
    }
   ],
   "source": [
    "# determimg the number of outliers\n",
    "clf = IsolationForest(contamination=0.005)\n",
    "X = df.iloc[:, 1:].dropna()\n",
    "y_pred = clf.fit_predict(X)\n",
    "outliers = np.where(y_pred==-1)\n",
    "outliers = outliers[0]\n",
    "print(outliers)\n",
    "print('Number of outliers:', len(outliers))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after dropping outliers: 98095\n"
     ]
    }
   ],
   "source": [
    "outliers_exist = df.index.isin(outliers)\n",
    "df_scaled = df[~outliers_exist]\n",
    "print('Number of rows after dropping outliers:', df_scaled.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Splitting and Preprocessing the Data<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['status']\n",
    "X = df.drop(columns=['status'])\n",
    "\n",
    "# use kfold cross validation\n",
    "kf = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 1: -942102547.6385761%\n",
      "Score for fold 2: -388821852.20946443%\n",
      "Score for fold 3: -643321757.1489563%\n",
      "Score for fold 4: -538967887.3481951%\n",
      "Score for fold 5: -737250944.5477134%\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "fold_no = 1\n",
    "for train, test, in kf.split(X, y):\n",
    "    X = scaler.fit_transform(X)\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y.iloc[train], y.iloc[test]\n",
    "    \n",
    "    # train the model\n",
    "    model = LocalOutlierFactor(n_neighbors=20, contamination=0.005, novelty=True)\n",
    "    y_pred = model.fit(X_train, y_train)\n",
    "    \n",
    "    # evaluate the model\n",
    "    score = model.score_samples(X_test)\n",
    "    # find the mean of the score\n",
    "    score = score.mean()\n",
    "    print(f'Score for fold {fold_no}: {score*100}%')\n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression scores: [0.64630992 0.65112858 0.64345135 0.6493355  0.64705286]\n",
      "Random Forest scores: [0.8333756  0.83028151 0.83062798 0.8296642  0.83291062]\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "logistic_scores = cross_val_score(logistic_model, X, y, cv=kfold)\n",
    "\n",
    "random_forest_model = RandomForestClassifier()\n",
    "\n",
    "random_forest_scores = cross_val_score(random_forest_model, X, y, cv=kfold)\n",
    "\n",
    "print(f'Logistic Regression scores: {logistic_scores}')\n",
    "print(f'Random Forest scores: {random_forest_scores}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for fold 1:\n",
      "Logistic Regression - Accuracy: 0.6499112351001776, Sensitivity: 0.6590125160145855, Specificity: 0.6402591973244147, AUC: 0.6496358566695002\n",
      "Random Forest - Accuracy: 0.8295206695409587, Sensitivity: 0.8471469399822608, Specificity: 0.8108277591973244, AUC: 0.8289873495897926\n",
      "\n",
      "Metrics for fold 2:\n",
      "Logistic Regression - Accuracy: 0.6453969059092062, Sensitivity: 0.6552717877372553, Specificity: 0.6351015333609614, AUC: 0.6451866605491084\n",
      "Random Forest - Accuracy: 0.8329190971341618, Sensitivity: 0.8569015204213455, Specificity: 0.807915457936179, AUC: 0.8324084891787622\n",
      "\n",
      "Metrics for fold 3:\n",
      "Logistic Regression - Accuracy: 0.646799228974333, Sensitivity: 0.6571657165716571, Specificity: 0.6361296963458569, AUC: 0.646647706458757\n",
      "Random Forest - Accuracy: 0.8284975144567313, Sensitivity: 0.8503850385038504, Specificity: 0.8059701492537313, AUC: 0.8281775938787909\n",
      "\n",
      "Metrics for fold 4:\n",
      "Logistic Regression - Accuracy: 0.6499949274627168, Sensitivity: 0.6547314578005116, Specificity: 0.6449518223711772, AUC: 0.6498416400858444\n",
      "Random Forest - Accuracy: 0.8301207263873389, Sensitivity: 0.8542199488491049, Specificity: 0.8044616673648932, AUC: 0.8293408081069991\n",
      "\n",
      "Metrics for fold 5:\n",
      "Logistic Regression - Accuracy: 0.644009333468601, Sensitivity: 0.6515734438196705, Specificity: 0.6360029236712959, AUC: 0.6437881837454832\n",
      "Random Forest - Accuracy: 0.8342802069595211, Sensitivity: 0.8565650586958666, Specificity: 0.8106922835961157, AUC: 0.8336286711459912\n",
      "\n",
      "Average Metrics:\n",
      "Logistic Regression - Accuracy: 0.6472223261830069, Sensitivity: 0.655550984388736, Specificity: 0.6384890346147413, AUC: 0.6470200095017387\n",
      "Random Forest - Accuracy: 0.8310676428957423, Sensitivity: 0.8530437012904857, Specificity: 0.8079734634696487, AUC: 0.8305085823800672\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "logistic_accuracy = []\n",
    "logistic_sensitivity = []\n",
    "logistic_specificity = []\n",
    "logistic_auc = []\n",
    "\n",
    "random_forest_accuracy = []\n",
    "random_forest_sensitivity = []\n",
    "random_forest_specificity = []\n",
    "random_forest_auc = []\n",
    "\n",
    "fold_no = 1\n",
    "for train, test in kf.split(X, y):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y.iloc[train], y.iloc[test]\n",
    "    \n",
    "    logistic_model.fit(X_train, y_train)\n",
    "    logistic_y_pred = logistic_model.predict(X_test)\n",
    "    \n",
    "    logistic_accuracy.append(accuracy_score(y_test, logistic_y_pred))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, logistic_y_pred).ravel()\n",
    "    logistic_sensitivity.append(tp / (tp + fn))\n",
    "    logistic_specificity.append(tn / (tn + fp))\n",
    "    logistic_auc.append(roc_auc_score(y_test, logistic_y_pred))\n",
    "    \n",
    "    random_forest_model.fit(X_train, y_train)\n",
    "    random_forest_y_pred = random_forest_model.predict(X_test)\n",
    "    \n",
    "    random_forest_accuracy.append(accuracy_score(y_test, random_forest_y_pred))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, random_forest_y_pred).ravel()\n",
    "    random_forest_sensitivity.append(tp / (tp + fn))\n",
    "    random_forest_specificity.append(tn / (tn + fp))\n",
    "    random_forest_auc.append(roc_auc_score(y_test, random_forest_y_pred))\n",
    "    \n",
    "    print(f\"Metrics for fold {fold_no}:\")\n",
    "    print(f\"Logistic Regression - Accuracy: {logistic_accuracy[-1]}, Sensitivity: {logistic_sensitivity[-1]}, Specificity: {logistic_specificity[-1]}, AUC: {logistic_auc[-1]}\")\n",
    "    print(f\"Random Forest - Accuracy: {random_forest_accuracy[-1]}, Sensitivity: {random_forest_sensitivity[-1]}, Specificity: {random_forest_specificity[-1]}, AUC: {random_forest_auc[-1]}\")\n",
    "    print()\n",
    "    \n",
    "    fold_no += 1\n",
    "\n",
    "print(\"Average Metrics:\")\n",
    "print(f\"Logistic Regression - Accuracy: {sum(logistic_accuracy) / len(logistic_accuracy)}, Sensitivity: {sum(logistic_sensitivity) / len(logistic_sensitivity)}, Specificity: {sum(logistic_specificity) / len(logistic_specificity)}, AUC: {sum(logistic_auc) / len(logistic_auc)}\")\n",
    "print(f\"Random Forest - Accuracy: {sum(random_forest_accuracy) / len(random_forest_accuracy)}, Sensitivity: {sum(random_forest_sensitivity) / len(random_forest_sensitivity)}, Specificity: {sum(random_forest_specificity) / len(random_forest_specificity)}, AUC: {sum(random_forest_auc) / len(random_forest_auc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.647222</td>\n",
       "      <td>0.655551</td>\n",
       "      <td>0.638489</td>\n",
       "      <td>0.647020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.831068</td>\n",
       "      <td>0.853044</td>\n",
       "      <td>0.807973</td>\n",
       "      <td>0.830509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  Sensitivity  Specificity       AUC\n",
       "0  Logistic Regression  0.647222     0.655551     0.638489  0.647020\n",
       "1        Random Forest  0.831068     0.853044     0.807973  0.830509"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest'],\n",
    "    'Accuracy': [sum(logistic_accuracy) / len(logistic_accuracy), sum(random_forest_accuracy) / len(random_forest_accuracy)],\n",
    "    'Sensitivity': [sum(logistic_sensitivity) / len(logistic_sensitivity), sum(random_forest_sensitivity) / len(random_forest_sensitivity)],\n",
    "    'Specificity': [sum(logistic_specificity) / len(logistic_specificity), sum(random_forest_specificity) / len(random_forest_specificity)],\n",
    "    'AUC': [sum(logistic_auc) / len(logistic_auc), sum(random_forest_auc) / len(random_forest_auc)]\n",
    "})\n",
    "metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: t-statistic = -86.09608185526017, p-value = 1.0910061690255404e-07\n",
      "Sensitivity: t-statistic = -65.25858928952692, p-value = 3.3030943093591973e-07\n",
      "AUC: t-statistic = -85.94937311415222, p-value = 1.0984709322043184e-07\n"
     ]
    }
   ],
   "source": [
    "t_statistic, p_value = stats.ttest_rel(logistic_accuracy, random_forest_accuracy)\n",
    "print(f'Accuracy: t-statistic = {t_statistic}, p-value = {p_value}')\n",
    "\n",
    "t_statistic, p_value = stats.ttest_rel(logistic_sensitivity, random_forest_sensitivity)\n",
    "print(f'Sensitivity: t-statistic = {t_statistic}, p-value = {p_value}')\n",
    "\n",
    "t_statistic, p_value = stats.ttest_rel(logistic_auc, random_forest_auc)\n",
    "print(f'AUC: t-statistic = {t_statistic}, p-value = {p_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "n_underscore: 0.3706620348989005\n",
      "n_exclamation: 0.2779316787644186\n",
      "status: 0.13786020383007686\n",
      "Logistic Regression:\n",
      "Index(['n_and', 'n_slash', 'n_underscore', 'url_length', 'n_equals',\n",
      "       'n_exclamation', 'n_at', 'n_questionmrk', 'status', 'n_period',\n",
      "       'n_hyphens'],\n",
      "      dtype='object'): [0.03054741 0.04013388 0.08637364 0.17258667 0.24022962 0.40019982\n",
      " 0.40975636 0.42976707 0.59861649 0.79453022 0.84656169]\n"
     ]
    }
   ],
   "source": [
    "# figure out what the best features are for each model, then print out the top 3 features\n",
    "logistic_model.fit(X, y)\n",
    "random_forest_model.fit(X, y)\n",
    "\n",
    "rf_importances = random_forest_model.feature_importances_\n",
    "\n",
    "lr_importances = np.abs(logistic_model.coef_)\n",
    "feature_names = df.columns\n",
    "\n",
    "\n",
    "\n",
    "print(\"Random Forest:\")\n",
    "for i in np.argsort(rf_importances)[::-1][:3]:\n",
    "    print(f\"{feature_names[i]}: {rf_importances[i]}\")\n",
    "\n",
    "\n",
    "print(\"Logistic Regression:\")\n",
    "for i in np.argsort(lr_importances)[::-1][:3]:\n",
    "    print(f\"{feature_names[i]}: {lr_importances[0][i]}\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
